# -*- coding: utf-8 -*-
"""Breast Cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17djnMfN7WaTl4lTBsRF23_b2GEUP40fh
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. CSV লোড
df = pd.read_csv("data.csv")

# 2. অপ্রয়োজনীয় কলাম ড্রপ করা
df = df.drop(columns=["id", "Unnamed: 32"])

# 3. Target encoding (M=1, B=0)
df["diagnosis"] = df["diagnosis"].map({"M": 1, "B": 0})

# 4. Feature / Target আলাদা করা
X = df.drop(columns=["diagnosis"])
y = df["diagnosis"]

# 5. Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 6. Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Train shape:", X_train_scaled.shape)
print("Test shape:", X_test_scaled.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt

df = pd.read_csv("data.csv")
df = df.drop(columns=["id", "Unnamed: 32"])
df["diagnosis"] = df["diagnosis"].map({"M": 1, "B": 0})

X = df.drop(columns=["diagnosis"])
y = df["diagnosis"]

# Add random noise feature
np.random.seed(42) # for reproducibility
X['noise'] = np.random.randn(len(X))


# Train-test split (adjust ratio and remove stratify)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42 # Changed test_size to 0.3 and removed stratify
)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled arrays back to DataFrames with column names for easier use with models
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, StratifiedKFold


lgbm = LGBMClassifier(
    random_state=42,
    max_depth=-1,             # Unlimited depth
    min_child_samples=1,      # ছোট ডেটাসেটে স্প্লিট করতে সাহায্য করবে
    min_child_weight=1e-3,
    num_leaves=31,            # ডিফল্ট ভালো মান
    learning_rate=0.05,
    n_estimators=500
)

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
lgbm_cv_accuracy = cross_val_score(lgbm, X_train_scaled, y_train, cv=cv, scoring='accuracy').mean()
lgbm_cv_auc = cross_val_score(lgbm, X_train_scaled, y_train, cv=cv, scoring='roc_auc').mean()
lgbm_cv_ap = cross_val_score(lgbm, X_train_scaled, y_train, cv=cv, scoring='average_precision').mean()

print(f"LightGBM Cross-validation Accuracy: {lgbm_cv_accuracy:.4f}")
print(f"LightGBM Cross-validation AUC-ROC: {lgbm_cv_auc:.4f}")
print(f"LightGBM Cross-validation Average Precision: {lgbm_cv_ap:.4f}")


lgbm.fit(X_train_scaled, y_train)
lgbm_pred = lgbm.predict(X_test_scaled)
lgbm_proba = lgbm.predict_proba(X_test_scaled)[:, 1]


print("\nLightGBM Test Set Performance:")
print("Accuracy:", accuracy_score(y_test, lgbm_pred))
print(classification_report(y_test, lgbm_pred))
print("AUC-ROC:", roc_auc_score(y_test, lgbm_proba))
print("Average Precision:", average_precision_score(y_test, lgbm_proba))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, lgbm_proba)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - LightGBM')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, lgbm_proba)
ap = average_precision_score(y_test, lgbm_proba)
plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (AP = {ap:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - LightGBM')
plt.legend(loc="lower left")
plt.show()

from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, StratifiedKFold

cat = CatBoostClassifier(verbose=0, random_state=42)

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cat_cv_accuracy = cross_val_score(cat, X_train_scaled, y_train, cv=cv, scoring='accuracy').mean()
cat_cv_auc = cross_val_score(cat, X_train_scaled, y_train, cv=cv, scoring='roc_auc').mean()
cat_cv_ap = cross_val_score(cat, X_train_scaled, y_train, cv=cv, scoring='average_precision').mean()

print(f"CatBoost Cross-validation Accuracy: {cat_cv_accuracy:.4f}")
print(f"CatBoost Cross-validation AUC-ROC: {cat_cv_auc:.4f}")
print(f"CatBoost Cross-validation Average Precision: {cat_cv_ap:.4f}")


cat.fit(X_train_scaled, y_train)
cat_pred = cat.predict(X_test_scaled)
cat_proba = cat.predict_proba(X_test_scaled)[:, 1]


print("\nCatBoost Test Set Performance:")
print("Accuracy:", accuracy_score(y_test, cat_pred))
print(classification_report(y_test, cat_pred))
print("AUC-ROC:", roc_auc_score(y_test, cat_proba))
print("Average Precision:", average_precision_score(y_test, cat_proba))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, cat_proba)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - CatBoost')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, cat_proba)
ap = average_precision_score(y_test, cat_proba)
plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (AP = {ap:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - CatBoost')
plt.legend(loc="lower left")
plt.show()

from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, StratifiedKFold

# ===== Model 3: MLP Neural Network =====
mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
mlp_cv_accuracy = cross_val_score(mlp, X_train_scaled, y_train, cv=cv, scoring='accuracy').mean()
mlp_cv_auc = cross_val_score(mlp, X_train_scaled, y_train, cv=cv, scoring='roc_auc').mean()
mlp_cv_ap = cross_val_score(mlp, X_train_scaled, y_train, cv=cv, scoring='average_precision').mean()

print(f"MLP Cross-validation Accuracy: {mlp_cv_accuracy:.4f}")
print(f"MLP Cross-validation AUC-ROC: {mlp_cv_auc:.4f}")
print(f"MLP Cross-validation Average Precision: {mlp_cv_ap:.4f}")


mlp.fit(X_train_scaled, y_train)
mlp_pred = mlp.predict(X_test_scaled)
mlp_proba = mlp.predict_proba(X_test_scaled)[:, 1]


print("\nMLP Test Set Performance:")
print("Accuracy:", accuracy_score(y_test, mlp_pred))
print(classification_report(y_test, mlp_pred))
print("AUC-ROC:", roc_auc_score(y_test, mlp_proba))
print("Average Precision:", average_precision_score(y_test, mlp_proba))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, mlp_proba)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - MLP')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, mlp_proba)
ap = average_precision_score(y_test, mlp_proba)
plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (AP = {ap:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - MLP')
plt.legend(loc="lower left")
plt.show()



from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, StratifiedKFold

# ===== Model 4: Quadratic Discriminant Analysis (QDA) =====
qda = QuadraticDiscriminantAnalysis(reg_param=0.01) # Added regularization parameter

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
qda_cv_accuracy = cross_val_score(qda, X_train_scaled, y_train, cv=cv, scoring='accuracy').mean()
qda_cv_auc = cross_val_score(qda, X_train_scaled, y_train, cv=cv, scoring='roc_auc').mean()
qda_cv_ap = cross_val_score(qda, X_train_scaled, y_train, cv=cv, scoring='average_precision').mean()

print(f"QDA Cross-validation Accuracy: {qda_cv_accuracy:.4f}")
print(f"QDA Cross-validation AUC-ROC: {qda_cv_auc:.4f}")
print(f"QDA Cross-validation Average Precision: {qda_cv_ap:.4f}")

qda.fit(X_train_scaled, y_train)
qda_pred = qda.predict(X_test_scaled)
qda_proba = qda.predict_proba(X_test_scaled)[:, 1]

print("\nQDA Test Set Performance:")
print("Accuracy:", accuracy_score(y_test, qda_pred))
print(classification_report(y_test, qda_pred))
print("AUC-ROC:", roc_auc_score(y_test, qda_proba))
print("Average Precision:", average_precision_score(y_test, qda_proba))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, qda_proba)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - QDA')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, qda_proba)
ap = average_precision_score(y_test, qda_proba)
plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (AP = {ap:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - QDA')
plt.legend(loc="lower left")
plt.show()

from sklearn.ensemble import VotingClassifier

# Identify the two best performing models based on previous analysis (MLP and CatBoost)
estimators = [('mlp', mlp), ('cat', cat)]

# Instantiate the VotingClassifier with 'soft' voting
ensemble_model = VotingClassifier(estimators=estimators, voting='soft')

ensemble_model.fit(X_train_scaled, y_train)

# Predict probabilities and labels
ensemble_proba = ensemble_model.predict_proba(X_test_scaled)[:, 1]
ensemble_pred = ensemble_model.predict(X_test_scaled)

# Calculate and print metrics
print("Ensemble Model Test Set Performance:")
print("Accuracy:", accuracy_score(y_test, ensemble_pred))
print(classification_report(y_test, ensemble_pred))
print("AUC-ROC:", roc_auc_score(y_test, ensemble_proba))
print("Average Precision:", average_precision_score(y_test, ensemble_proba))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, ensemble_proba)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Ensemble Model')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, ensemble_proba)
ap = average_precision_score(y_test, ensemble_proba)
plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (AP = {ap:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Ensemble Model')
plt.legend(loc="lower left")
plt.show()

performance_summary = {
    'LightGBM': {
        'Accuracy': accuracy_score(y_test, lgbm_pred),
        'AUC-ROC': roc_auc_score(y_test, lgbm_proba),
        'Average Precision': average_precision_score(y_test, lgbm_proba)
    },
    'CatBoost': {
        'Accuracy': accuracy_score(y_test, cat_pred),
        'AUC-ROC': roc_auc_score(y_test, cat_proba),
        'Average Precision': average_precision_score(y_test, cat_proba)
    },
    'MLP': {
        'Accuracy': accuracy_score(y_test, mlp_pred),
        'AUC-ROC': roc_auc_score(y_test, mlp_proba),
        'Average Precision': average_precision_score(y_test, mlp_proba)
    },
    'QDA': {
        'Accuracy': accuracy_score(y_test, qda_pred),
        'AUC-ROC': roc_auc_score(y_test, qda_proba),
        'Average Precision': average_precision_score(y_test, qda_proba)
    },
    'Ensemble': {
        'Accuracy': accuracy_score(y_test, ensemble_pred),
        'AUC-ROC': roc_auc_score(y_test, ensemble_proba),
        'Average Precision': average_precision_score(y_test, ensemble_proba)
    }
}

# Display the summary
print("Performance Summary of Models on Test Set:")
for model_name, metrics in performance_summary.items():
    print(f"\n--- {model_name} ---")
    for metric_name, value in metrics.items():
        print(f"{metric_name}: {value:.4f}")

import matplotlib.pyplot as plt
import numpy as np

model_names = list(performance_summary.keys())
accuracy_values = [performance_summary[model]['Accuracy'] for model in model_names]

plt.figure(figsize=(10, 6))
plt.bar(model_names, accuracy_values, color=['blue', 'green', 'red', 'purple', 'orange'])
plt.ylim(0.9, 1.0) # Set y-axis limits to better visualize differences
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison on Test Set')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
sns.countplot(x='diagnosis', data=df)
plt.title('Distribution of Diagnosis (Malignant vs. Benign)')
plt.xticks([0, 1], ['Benign', 'Malignant']) # Assuming 0=Benign, 1=Malignant based on previous code
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select a few relevant numerical features for visualization
key_features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean']

# Create a figure and a set of subplots
fig, axes = plt.subplots(len(key_features), 1, figsize=(8, 4 * len(key_features)))

# Iterate through the selected features and create a histogram for each
for i, feature in enumerate(key_features):
    sns.histplot(data=df, x=feature, ax=axes[i], kde=True)
    axes[i].set_title(f'Distribution of {feature}')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Frequency')

# Adjust layout
plt.tight_layout()

# Display the plots
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# 1. Calculate the correlation matrix
correlation_matrix = df.corr()

# 2. Create a heatmap of the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f")

# 3. Set the title of the heatmap
plt.title('Feature Correlation Matrix')

# 4. Display the heatmap
plt.show()

"""In the visualization of the target variable, '0' represents a Benign diagnosis and '1' represents a Malignant diagnosis. This mapping was done in the data preprocessing step."""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the confusion matrix
cm = confusion_matrix(y_test, ensemble_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Ensemble Model')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt

df = pd.read_csv("data.csv")
df = df.drop(columns=["id", "Unnamed: 32"])
df["diagnosis"] = df["diagnosis"].map({"M": 1, "B": 0})

X = df.drop(columns=["diagnosis"])
y = df["diagnosis"]

# Add random noise feature
np.random.seed(42) # for reproducibility
X['noise'] = np.random.randn(len(X))


# Train-test split (adjust ratio and remove stratify)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42 # Changed test_size to 0.3 and removed stratify
)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled arrays back to DataFrames with column names for easier use with models
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

from sklearn.ensemble import VotingClassifier

# Identify the two best performing models based on previous analysis (MLP and CatBoost)
estimators = [('mlp', mlp), ('cat', cat)]

# Instantiate the VotingClassifier with 'soft' voting
ensemble_model = VotingClassifier(estimators=estimators, voting='soft')

ensemble_model.fit(X_train_scaled, y_train)

# Predict probabilities and labels
ensemble_proba = ensemble_model.predict_proba(X_test_scaled)[:, 1]
ensemble_pred = ensemble_model.predict(X_test_scaled)

# Calculate and print metrics
print("Ensemble Model Test Set Performance:")
print("Accuracy:", accuracy_score(y_test, ensemble_pred))
print(classification_report(y_test, ensemble_pred))
print("AUC-ROC:", roc_auc_score(y_test, ensemble_proba))
print("Average Precision:", average_precision_score(y_test, ensemble_proba))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, ensemble_proba)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Ensemble Model')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, ensemble_proba)
ap = average_precision_score(y_test, ensemble_proba)
plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (AP = {ap:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Ensemble Model')
plt.legend(loc="lower left")
plt.show()
